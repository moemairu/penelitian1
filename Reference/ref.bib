@article{10.1145/3443420,
abstract = {Rust is a relatively new programming language that has gained significant traction since its v1.0 release in 2015. Rust aims to be a systems language that competes with C/C++. A claimed advantage of Rust is a strong focus on memory safety without garbage collection. This is primarily achieved through two concepts, namely, reference lifetimes and borrowing. Both of these are well-known ideas stemming from the literature on region-based memory management and linearity/uniqueness. Rust brings both of these ideas together to form a coherent programming model. Furthermore, Rust has a strong focus on stack-allocated data and, like C/C++ but unlike Java, permits references to local variables.Type checking in Rust can be viewed as a two-phase process: First, a traditional type checker operates in a flow-insensitive fashion; second, a borrow checker enforces an ownership invariant using a flow-sensitive analysis. In this article, we present a lightweight formalism that captures these two phases using a flow-sensitive type system that enforces “type and borrow safety.” In particular, programs that are type and borrow safe will not attempt to dereference dangling pointers. Our calculus core captures many aspects of Rust, including copy- and move-semantics, mutable borrowing, reborrowing, partial moves, and lifetimes. In particular, it remains sufficiently lightweight to be easily digested and understood and, we argue, still captures the salient aspects of reference lifetimes and borrowing. Furthermore, extensions to the core can easily add more complex features (e.g., control-flow, tuples, method invocation). We provide a soundness proof to verify our key claims of the calculus. We also provide a reference implementation in Java with which we have model checked our calculus using over 500B input programs. We have also fuzz tested the Rust compiler using our calculus against 2B programs and, to date, found one confirmed compiler bug and several other possible issues.},
address = {New York, NY, USA},
author = {Pearce, David J},
doi = {10.1145/3443420},
file = {:C\:/Users/ASUS/Downloads/Referensi/3443420.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearce - 2021 - A Lightweight Formalism for Reference Lifetimes and Borrowing in Rust.pdf:pdf},
issn = {0164-0925},
journal = {ACM Trans. Program. Lang. Syst.},
keywords = {Rust,model checking,ownership,type theory},
month = {apr},
number = {1},
publisher = {Association for Computing Machinery},
title = {{A Lightweight Formalism for Reference Lifetimes and Borrowing in Rust}},
url = {https://doi.org/10.1145/3443420},
volume = {43},
year = {2021}
}
@article{Big2021,
abstract = {This article proposes a new parallel performance model for different workloads of Spark Big Data applications running on Hadoop clusters. The proposed model can predict the runtime for generic workloads as a function of the number of executors, without necessarily knowing how the algorithms were implemented. For a certain problem size, it is shown that a model based on serial boundaries for a 2D arrangement of executors can fit the empirical data for various workloads. The empirical data was obtained from a real Hadoop cluster, using Spark and HiBench. The workloads used in this work were included WordCount, SVM, Kmeans, PageRank and Graph (Nweight). A particular runtime pattern emerged when adding more executors to run a job. For some workloads, the runtime was longer with more executors added. This phenomenon is predicted with the new model of parallelisation. The resulting equation from the model explains certain performance patterns that do not fit Amdahl's law predictions, nor Gustafson's equation. The results show that the proposed model achieved the best fit with all workloads and most of the data sizes, using the R-squared metric for the accuracy of the fitting of empirical data. The proposed model has advantages over machine learning models due to its simplicity, requiring a smaller number of experiments to fit the data. This is very useful to practitioners in the area of Big Data because they can predict runtime of specific applications by analysing the logs. In this work, the model is limited to changes in the number of executors for a fixed problem size.},
author = {Ahmed, N and Barczak, Andre L.C. and Rashid, Mohammad A and Susnjak, Teo},
doi = {10.1186/s40537-021-00499-7},
file = {:C\:/Users/ASUS/Downloads/Referensi/s40537-021-00499-7.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Big et al. - 2021 - A parallelization model for performance characterization of Spark Big Data jobs on Hadoop clusters.pdf:pdf},
isbn = {4053702100499},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big Data,HiBench,Performance prediction,Spark,System configuration},
month = {dec},
number = {1},
pages = {107},
publisher = {Springer International Publishing},
title = {{A parallelization model for performance characterization of Spark Big Data jobs on Hadoop clusters}},
url = {https://doi.org/10.1186/s40537-021-00499-7 https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00499-7},
volume = {8},
year = {2021}
}
@article{PONS2025105091,
abstract = {Resource management has become a major concern in dealing with performance and fairness in recent computing servers, including a wide variety of shared resources. To achieve high-performing and efficient systems, both hardware and software engineers must be thoroughly trained in effective resource management techniques. This paper introduces the GRE master course (Spanish acronym for Resource Management and Performance Evaluation in Cloud and High-Performance Workloads), which is being offered since Fall 2023. The course is taught by instructors with broad research expertise in resource management and performance evaluation. Subjects covered in this course include workload characterization, state-of-the-art resource management approaches, and performance evaluation tools and methodologies used in production systems. Management techniques are studied both in the context of HPC and cloud computing, where resource efficiency is becoming a primary concern. To enhance the learning experience, the course integrates theoretical concepts with a wide set of hands-on tasks carried out on recent real platforms. A real cloud virtualized environment is mimicked using typical software deployed in production systems such as Proxmox Virtual Environment. Students learn to use tools such as Linux Perf and Intel Vtune Profiler, which are commonly employed by researchers and practitioners to carry out typical tasks like performance bottleneck analysis from a microarchitectural perspective. Overall, the GRE course provides students with a solid foundation and skills in resource management by addressing current hot topics both in the industry and academia. Student satisfaction and learning outcomes prove the success of the GRE course and encourage us to continue in this direction.},
author = {Pons, Lucia and Petit, Salvador and Sahuquillo, Julio},
doi = {https://doi.org/10.1016/j.jpdc.2025.105091},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731525000589-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pons, Petit, Sahuquillo - 2025 - Advanced resource management A hands-on master course in HPC and cloud computing.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Cloud computing,Computer architecture,High-performance computing,Lab sessions,Performance evaluation,Resource management},
pages = {105091},
title = {{Advanced resource management: A hands-on master course in HPC and cloud computing}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000589},
volume = {202},
year = {2025}
}
@article{https://doi.org/10.1049/iet-cdt.2018.5220,
abstract = {For over 50 years, Amdahl's Law has been the hallmark model for reasoning about performance bounds for homogeneous parallel computing resources. As heterogeneous, many-core parallel resources continue to permeate into the modern server and embedded domains, there has been growing interest in promulgating realistic extensions and assumptions in keeping with newer use cases. This study aims to provide a comprehensive review of the purviews and insights provided by the extensive body of work related to Amdahl's law to date, focusing on computation speedup. The authors show that a significant portion of these studies has looked into analysing the scalability of the model considering both workload and system heterogeneity in real-world applications. The focus has been to improve the definition and semantic power of the two key parameters in the original model: the parallel fraction (f) and the computation capability improvement index (n). More recently, researchers have shown normal-form and multi-fraction extensions that can account for wider ranges of heterogeneity, validated on many-core systems running realistic workloads. Speedup models from Amdahl's law onwards have seen a wide range of uses, such as the optimisation of system execution, and these uses are even more important with the advent of the heterogeneous many-core era.},
author = {Al-hayanni, Mohammed A Noaman and Xia, Fei and Rafiev, Ashur and Romanovsky, Alexander and Shafik, Rishad and Yakovlev, Alex},
doi = {https://doi.org/10.1049/iet-cdt.2018.5220},
file = {:C\:/Users/ASUS/Downloads/Referensi/IET Computers Digital Tech - 2020 - Al‐hayanni - Amdahl s law in the context of heterogeneous many‐core systems a.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Al-hayanni et al. - 2020 - Amdahl's law in the context of heterogeneous many-core systems – a survey.pdf:pdf},
journal = {IET Computers \& Digital Techniques},
keywords = {Amdahl's law onwards,computation capability improvement index,computation speedup,hallmark model,heterogeneous core parallel resources,heterogeneous many-core era,heterogeneous many-core systems,homogeneous parallel computing resources,many-core parallel resources,multifraction extensions,multiprocessing systems,parallel fraction,parallel processing,power aware computing,speedup models,system execution,system heterogeneity},
number = {4},
pages = {133--148},
title = {{Amdahl's law in the context of heterogeneous many-core systems – a survey}},
url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-cdt.2018.5220},
volume = {14},
year = {2020}
}
@article{NEWHALL2025105044,
abstract = {We present the curricular design, pedagogy, and goals of an introductory-level course on computer systems that introduces parallel and distributed computing (PDC) to students who have only a CS1 background. With the ubiquity of multicore processors, cloud computing, and hardware accelerators, PDC topics have become fundamental knowledge areas in the undergraduate CS curriculum. As a result, it is increasingly important for students to learn a common core of introductory parallel and distributed computing topics and to develop parallel thinking skills early in their CS studies. Our introductory-level course focuses on three main curricular goals: 1) understanding how a computer runs a program, 2) evaluating system costs associated with running a program, and 3) taking advantage of the power of parallel computing. We elaborate on the goals and details of our course's key modules, and we discuss our pedagogical approach that includes active-learning techniques. We also include an evaluation of our course and a discussion of our experiences teaching it since Fall 2012. We find that the PDC foundation gained through early exposure in our course helps students gain confidence in their ability to expand and apply their understanding of PDC concepts throughout their CS education.},
author = {Newhall, Tia and Webb, Kevin C and Chaganti, Vasanta and Danner, Andrew},
doi = {https://doi.org/10.1016/j.jpdc.2025.105044},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731525000115-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newhall et al. - 2025 - An introductory-level undergraduate CS course that introduces parallel computing.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {CS curriculum,Introductory CS,Parallel computing},
pages = {105044},
title = {{An introductory-level undergraduate CS course that introduces parallel computing}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000115},
volume = {199},
year = {2025}
}
@inproceedings{10.1007/978-3-031-48803-0_11,
abstract = {Many scientific high performance codes that simulate e.g. black holes, coastal waves, climate and weather, etc. rely on block-structured meshes and use finite differencing methods to solve the appropriate systems of differential equations iteratively. This paper investigates implementations of a straightforward simulation of this type using various programming systems and languages. We focus on a shared memory, parallelized algorithm that simulates a 1D heat diffusion using asynchronous queues for the ghost zone exchange. We discuss the advantages of the various platforms and explore the performance of this model code on different computing architectures: Intel, AMD, and ARM64FX. As a result, Python was the slowest of the set we compared. Java, Go, Swift, and Julia were the intermediate performers. The higher performing platforms were C++, Rust, Chapel, Charm++, and HPX.},
address = {Cham},
author = {Diehl, Patrick and Morris, Max and Brandt, Steven R and Gupta, Nikunj and Kaiser, Hartmut},
booktitle = {Euro-Par 2023: Parallel Processing Workshops},
doi = {10.1007/978-3-031-48803-0_11},
editor = {Zeinalipour, Demetris and {Blanco Heras}, Dora and Pallis, George and Herodotou, Herodotos and Trihinas, Demetris and Balouek, Daniel and Diehl, Patrick and Cojean, Terry and F{\"{u}}rlinger, Karl and Kirkeby, Maja Hanne and Nardelli, Matteo and {Di Sanzo}, Pierangelo},
file = {:C\:/Users/ASUS/Downloads/Penelitian 1/Referensi/Ignore/Diehl et al. - 2024 - Benchmarking the Parallel 1D Heat Equation Solver in Chapel, Charm++, C++, HPX, Go, Julia, Python, Rust, Swift, and Java.pdf:pdf},
isbn = {978-3-031-48803-0},
pages = {127--138},
publisher = {Springer Nature Switzerland},
title = {{Benchmarking the Parallel 1D Heat Equation Solver in Chapel, Charm++, C++, HPX, Go, Julia, Python, Rust, Swift, and Java}},
url = {https://link.springer.com/10.1007/978-3-031-48803-0_11},
year = {2024}
}
@article{Mroczek_Mańturz_Miłosz_2025,
abstract = {This article presents results of exploration of the combined application of Python and Rust in enhancing software performance, focusing on algorithm implementation. Python, known for its simplicity and flexibility, is widely used in various domains but faces limitations in CPU-intensive tasks. Rust, on the other hand, excels in performance and safety of memory management, making it a compelling choice for optimizing critical code sections. By leveraging tools such as PyO3 and Maturin, Authors examine how Rust's compiled code can be seamlessly integrated into Python projects to mitigate performance bottlenecks.},
author = {Mroczek, Przemys{\l}aw and Ma{\'{n}}turz, Jakub and Mi{\l}osz, Marek},
doi = {10.35784/jcsi.7050},
file = {:C\:/Users/ASUS/Downloads/Penelitian 1/Referensi/Ignore/Mroczek et al. - 2025 - Comparative analysis of Python and Rust evaluating their combined impact on performance.pdf:pdf},
issn = {2544-0764},
journal = {Journal of Computer Sciences Institute},
month = {jun},
pages = {137--141},
title = {{Comparative analysis of Python and Rust: evaluating their combined impact on performance}},
url = {https://ph.pollub.pl/index.php/jcsi/article/view/7050},
volume = {35},
year = {2025}
}
@article{Abhinav2020,
abstract = {There has been tremendous progress in the past few decades towards developing applications that receive data and send data concurrently. In such a day and age, there is a requirement for a language that can perform optimally in such environments. Currently, the two most popular languages in that respect are Go and Java. In this paper, we look to analyze the concurrency features of Go and Java through a complete programming language performance analysis, looking at their compile time, run time, binary sizes and the language's unique concurrency features. This is done by experimenting with the two languages using the matrix multiplication and PageRank algorithms. To the extent of our knowledge, this is the first work which used PageRank algorithm to analyse concurrency. Considering the results of this paper, application developers and researchers can hypothesize on an appropriate language to use for their concurrent programming activity.Results of this paper show that Go performs better for fewer number of computation but is soon taken over by Java as the number of computations drastically increase. This trend is shown to be the opposite when thread creation and management is considered where Java performs better with fewer computation but Go does better later on. Regarding concurrency features both Java with its Executor Service library and Go had their own advantages that made them better for specific applications.},
author = {Abhinav, P Y and Bhat, Avakash and Joseph, Christina Terese and Chandrasekaran, K},
doi = {10.1109/ICCCS49678.2020.9277498},
file = {:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abhinav et al. - 2020 - Concurrency analysis of go and java.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abhinav et al. - 2020 - Concurrency analysis of go and java(2).pdf:pdf},
isbn = {9781728191805},
journal = {Proceedings of the 2020 International Conference on Computing, Communication and Security, ICCCS 2020},
keywords = {Concurrency,Go,Java,Matrix Multiplication,Pagerank},
title = {{Concurrency analysis of go and java}},
year = {2020}
}
@inproceedings{Yuan2020,
abstract = {Despite their wide deployment, distributed systems remain notoriously hard to reason about. Unexpected interleavings of concurrent operations and failures may lead to undefined behaviors and cause serious consequences. We present Morpheus, the first concurrency testing tool leveraging partial order sampling, a randomized testing method formally analyzed and empirically validated to provide strong probabilistic guarantees of error-detection, for real-world distributed systems. Morpheus introduces conflict analysis to further improve randomized testing by predicting and focusing on operations that affect the testing result. Inspired by the recent shift in building distributed systems using higher-level languages and frameworks, Morpheus targets Erlang. Evaluation on four popular distributed systems in Erlang including RabbitMQ, a message broker service, and Mnesia, a distributed database in the Erlang standard libraries, shows that Morpheus is effective: It found previously unknown errors in every system checked, 11 total, all of which are flaws in their core protocols that may cause deadlocks, unexpected crashes, or inconsistent states.},
address = {New York, NY, USA},
author = {Yuan, Xinhao and Yang, Junfeng},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
doi = {10.1145/3373376.3378484},
file = {:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Yang - 2020 - Effective concurrency testing for distributed systems.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Yang - 2020 - Effective Concurrency Testing for Distributed Systems(2).pdf:pdf},
isbn = {9781450371025},
keywords = {Conflict analysis,Distributed systems,Partial order sampling,Partial-order reduction,Randomized testing},
month = {mar},
pages = {1141--1156},
publisher = {ACM},
title = {{Effective Concurrency Testing for Distributed Systems}},
url = {https://dl.acm.org/doi/10.1145/3373376.3378484},
year = {2020}
}
@article{10.1145/3591284,
abstract = {Although functional programming languages simplify writing safe parallel programs by helping programmers to avoid data races, they have traditionally delivered poor performance. Recent work improved performance by using a hierarchical memory architecture that allows processors to allocate and reclaim memory independently without any synchronization, solving thus the key performance challenge afflicting functional programs. The approach, however, restricts mutation, or memory effects, so as to ensure "disentanglement", a low-level memory property that guarantees independence between different heaps in the hierarchy. This paper proposes techniques for supporting entanglement and for allowing functional programs to use mutation at will. Our techniques manage entanglement by distinguishing between disentangled and entangled objects and shielding disentangled objects from the cost of entanglement management. We present a semantics that formalizes entanglement as a property at the granularity of memory objects, and define several cost metrics to reason about and bound the time and space cost of entanglement. We present an implementation of the techniques by extending the MPL compiler for Parallel ML. The extended compiler supports all features of the Parallel ML language, including unrestricted effects. Our experiments using a variety of benchmarks show that MPL incurs a small time and space overhead compared to sequential runs, scales well, and is competitive with languages such as C++, Go, Java, OCaml. These results show that our techniques can marry the safety benefits of functional programming with performance.},
address = {New York, NY, USA},
author = {Arora, Jatin and Westrick, Sam and Acar, Umut A},
doi = {10.1145/3591284},
file = {:C\:/Users/ASUS/Downloads/Referensi/3591284.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Westrick, Acar - 2023 - Efficient Parallel Functional Programming with Effects.pdf:pdf},
journal = {Proc. ACM Program. Lang.},
keywords = {concurrent,functional programming,memory management,parallel},
month = {jun},
number = {PLDI},
publisher = {Association for Computing Machinery},
title = {{Efficient Parallel Functional Programming with Effects}},
url = {https://doi.org/10.1145/3591284},
volume = {7},
year = {2023}
}
@article{QUISLANT2024104855,
abstract = {Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, etc. Matrix Profile, a state-of-the-art algorithm to perform time series analysis, finds out the most similar and dissimilar subsequences in a time series in deterministic time and it is exact. Matrix Profile has low arithmetic intensity and it operates on large amounts of time series data, which can be an issue in terms of memory requirements. On the other hand, Hardware Transactional Memory (HTM) is an alternative optimistic synchronization method that executes transactions speculatively in parallel while keeping track of memory accesses to detect and resolve conflicts. This work evaluates one of the best implementations of Matrix Profile exploring multiple multiprocessor variants and proposing new implementations that consider a variety of synchronization methods (HTM, locks, barriers), as well as algorithm organizations. We analyze these variants using real datasets, both short and large, in terms of speedup and memory requirements, the latter being a major issue when dealing with very large time series. The experimental evaluation shows that our proposals can achieve up to 100× speedup over the sequential algorithm for 128 threads, and up to 3× over the baseline, while keeping memory requirements low and even independent of the number of threads.},
author = {Quislant, Ricardo and Gutierrez, Eladio and Plata, Oscar},
doi = {https://doi.org/10.1016/j.jpdc.2024.104855},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731524000194-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quislant, Gutierrez, Plata - 2024 - Exploring multiprocessor approaches to time series analysis.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Hardware transactional memory,Intel Restricted Transactional Memory (RTM),Matrix profile,Shared-memory parallelism,Time series analysis},
pages = {104855},
title = {{Exploring multiprocessor approaches to time series analysis}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731524000194},
volume = {188},
year = {2024}
}
@inproceedings{10894767,
author = {Morshed, Md. Neaz and Roy, Prince},
booktitle = {2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)},
doi = {10.1109/ICAC2N63387.2024.10894767},
file = {:C\:/Users/ASUS/Downloads/Referensi/jcsi35(2025)137-141.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morshed, Roy - 2024 - Go vs. Java A Detailed Performance Analysis in Concurrent Programming.pdf:pdf},
keywords = {Computer languages,Concurrent computing,Java,Runti},
pages = {1800--1804},
title = {{Go vs. Java: A Detailed Performance Analysis in Concurrent Programming}},
year = {2024}
}
@article{10.1145/3617687,
abstract = {Actor frameworks and similar reactive programming techniques are widely used for building concurrent systems. They promise to be efficient and scale well to a large number of cores or nodes in a distributed system. However, they also expose programmers to nondeterminism, which often makes implementations hard to understand, debug, and test. The recently proposed reactor model is a promising alternative that enables deterministic concurrency. In this article, we present an efficient, parallel implementation of reactors and demonstrate that the determinacy of reactors does not imply a loss in performance. To show this, we evaluate Lingua Franca (LF), a reactor-oriented coordination language. LF equips mainstream programming languages with a deterministic concurrency model that automatically takes advantage of opportunities to exploit parallelism. Our implementation of the Savina benchmark suite demonstrates that, in terms of execution time, the runtime performance of LF programs even exceeds popular and highly optimized actor frameworks. We compare against Akka and CAF, which LF outperforms by 1.86\texttimes{} and 1.42\texttimes{}, respectively.},
address = {New York, NY, USA},
author = {Menard, Christian and Lohstroh, Marten and Bateni, Soroush and Chorlian, Matthew and Deng, Arthur and Donovan, Peter and Fournier, Cl{\'{e}}ment and Lin, Shaokai and Suchert, Felix and Tanneberger, Tassilo and Kim, Hokeun and Castrillon, Jeronimo and Lee, Edward A},
doi = {10.1145/3617687},
file = {:C\:/Users/ASUS/Downloads/Referensi/3617687.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Menard et al. - 2023 - High-performance Deterministic Concurrency Using Lingua Franca.pdf:pdf},
issn = {1544-3566},
journal = {ACM Trans. Archit. Code Optim.},
keywords = {Coordination,concurrency,determinism,performance},
month = {oct},
number = {4},
publisher = {Association for Computing Machinery},
title = {{High-performance Deterministic Concurrency Using Lingua Franca}},
url = {https://doi.org/10.1145/3617687},
volume = {20},
year = {2023}
}
@article{10.1145/3648439,
abstract = {Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.},
address = {New York, NY, USA},
author = {Din, Crystal Chang and H{\"{a}}hnle, Reiner and Henrio, Ludovic and Johnsen, Einar Broch and Pun, Violet Ka I and Tarifa, S Lizeth Tapia},
doi = {10.1145/3648439},
file = {:C\:/Users/ASUS/Downloads/Referensi/3648439.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Din et al. - 2024 - Locally Abstract, Globally Concrete Semantics of Concurrent Programming Languages.pdf:pdf},
issn = {0164-0925},
journal = {ACM Trans. Program. Lang. Syst.},
keywords = {Denotational semantics,compositionality,concurrent programming languages,continuations,program calculus,program logics,trace semantics},
month = {mar},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Locally Abstract, Globally Concrete Semantics of Concurrent Programming Languages}},
url = {https://doi.org/10.1145/3648439},
volume = {46},
year = {2024}
}
@article{CARDENAS2025105105,
abstract = {This paper presents a new algorithm for the Discrete EVent System Specification (DEVS) formalism that improves the performance of simulating complex systems by reducing the number of iterations through the model components in each simulation step. It also minimizes unnecessary visits to model components by propagating simulation routines only when necessary. Additionally, we provide two parallel versions of this new simulation algorithm that use work-stealing scheduling and avoid locking mechanisms without compromising the validity of the execution in shared-memory architectures. We implemented the proposed algorithms in the xDEVS simulator and evaluated their performance using the DEVStone synthetic benchmark. The results show that the proposed algorithms outperform state-of-the-art alternatives. For computationally intensive models, parallel implementations achieve high parallelism efficiency. Furthermore, they are more resilient to model complexity than the sequential algorithm, showing better performance for complex models even without computational overhead in state transition functions.},
author = {C{\'{a}}rdenas, Rom{\'{a}}n and Arroba, Patricia and Risco-Mart{\'{i}}n, Jos{\'{e}} L},
doi = {https://doi.org/10.1016/j.jpdc.2025.105105},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731525000723-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/C{\'{a}}rdenas, Arroba, Risco-Mart{\'{i}}n - 2025 - Lock-free simulation algorithm to enhance the performance of sequential and parallel DEVS simula.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Discrete-event simulation,Lock-free parallelism,Parallel simulation},
pages = {105105},
title = {{Lock-free simulation algorithm to enhance the performance of sequential and parallel DEVS simulators in shared-memory architectures}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000723},
volume = {203},
year = {2025}
}
@article{Chaudhary2025,
abstract = {Academic Information Technology (IT) programs often introduce or revise courses that teach programming languages. They name these courses differently. A title frequently mentioned in these courses is “modern programming languages.” Although the word “modern” may sound like the most recent, the actual aspects of what programming languages meet the definition of “modern” may need clarification. The set of requirements for classifying a programming language as “modern” may need to be defined more clearly, and instructors may ensure that all features are at least explicated in some detail, even if they are not within the scope of a particular course in a language. This paper reviews features and criteria that may need to be fulfilled by a programming language to be considered as “modern”. It is expected that most languages will fulfill all these criteria. However, their strengths will vary across different features. Instructors and practitioners may need to rank order the most important criteria for selection of a language for instruction and development purposes.},
author = {Chaudhary, Pankaj and Agrawal, Lavlin and Ali, Azad},
doi = {10.48009/2_iis_122},
file = {:C\:/Users/ASUS/Downloads/Referensi/2_iis_2025_281-291.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaudhary, Agrawal, Ali - 2025 - Modern Programming Languages Characteristics and Recommendations for Instruction.pdf:pdf},
issn = {15297314},
journal = {Issues In Information Systems},
keywords = {computer programming,modern programming languages.,programming courses},
number = {2},
pages = {281--291},
title = {{Modern Programming Languages ? Characteristics and Recommendations for Instruction}},
url = {https://iacis.org/iis/2025/2_iis_2025_281-291.pdf},
volume = {26},
year = {2025}
}
@article{9965751,
author = {Rogowski, Marcin and Aseeri, Samar and Keyes, David and Dalcin, Lisandro},
doi = {10.1109/TPDS.2022.3225481},
file = {:C\:/Users/ASUS/Downloads/Referensi/mpi4py.futures_MPI-based_asynchronous_task_execution_for_Python.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rogowski et al. - 2023 - mpi4py.futures MPI-Based Asynchronous Task Execution for Python.pdf:pdf},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Codes,Libraries,MPI,Message systems,Python,Sockets,Standards,Task analysis,distributed computing,high performance computing,master-worker,multiprocessing,parallel programming models,parallelism,task execution},
number = {2},
pages = {611--622},
title = {{mpi4py.futures: MPI-Based Asynchronous Task Execution for Python}},
volume = {34},
year = {2023}
}
@article{POOLLA2023104745,
abstract = {The problem of learning parallel computer performance is investigated in the context of multicore processors. Given a fixed workload, the effect of varying system configuration on performance is sought. Conventionally, the performance speedup due to a single resource enhancement is formulated using Amdahl's law. However, in case of multiple configurable resources the conventional formulation results in several disconnected speedup equations that cannot be combined together to determine the overall speedup. To solve this problem, we propose to (1) extend Amdahl's law to accommodate multiple configurable resources into the overall speedup equation, and (2) transform the speedup equation into a multivariable regression problem suitable for machine learning. Using experimental data from fifty-eight tests spanning two benchmarks (SPECCPU 2017 and PCMark 10) and four hardware platforms (Intel Xeon 8180M, AMD EPYC 7702P, Intel CoffeeLake 8700K, and AMD Ryzen 3900X), analytical models are developed and cross-validated. Findings indicate that in most cases, the models result in an average cross-validated accuracy higher than 95%, thereby validating the proposed extension of Amdahl's law. The proposed methodology enables rapid generation of multivariable analytical models to support future industrial development, optimization, and simulation needs.},
author = {Poolla, Chaitanya and Saxena, Rahul},
doi = {https://doi.org/10.1016/j.micpro.2022.104745},
file = {:C\:/Users/ASUS/Downloads/Referensi/2110.07822v2.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poolla, Saxena - 2023 - On extending Amdahl's law to learn computer performance.pdf:pdf},
issn = {0141-9331},
journal = {Microprocessors and Microsystems},
keywords = {Amdahl's law,Analytical modeling,Computer performance,Machine learning,Regression},
pages = {104745},
title = {{On extending Amdahl's law to learn computer performance}},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002745},
volume = {96},
year = {2023}
}
@article{10.1145/3774654,
abstract = {General Sparse Matrix-Matrix Multiplication (SpGEMM) is a crucial computational kernel in the field of scientific and engineering computing. Due to the irregular distribution of nonzero elements in sparse matrices, SpGEMM computation faces challenges such as non-contiguous memory access and workload imbalance. This paper focuses on optimizing SpGEMM for GPU platforms. First, a lightweight machine learning model is trained to predict the optimal method for estimating the size of result matrix. Next, different kernels are launched in groups to maximize GPU shared memory utilization and achieve load balancing. For the hash-based sparse accumulator, heuristic methods are used to select the optimal hash load factors and hash multiplier factors, thereby reducing the number of hash collisions. In addition, thread reduction is applied in the symbolic phase to enhance intra-block parallelism. Combining these optimization strategies, we implemented an adaptive SpGEMM algorithm for GPUs and compared its performance with current state-of-the-art algorithms. The results show that our algorithm achieves significant performance improvements.},
address = {New York, NY, USA},
annote = {Just Accepted},
author = {Wang, Yizhuo and Lin, Hongpeng and Wei, Bingxin and Gao, Jianhua and Ji, Weixing},
doi = {10.1145/3774654},
file = {:C\:/Users/ASUS/Downloads/Referensi/3774654.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2025 - Optimizing General Sparse Matrix-Matrix Multiplication on the GPU.pdf:pdf},
issn = {1544-3566},
journal = {ACM Trans. Archit. Code Optim.},
keywords = {GPU,Parallel Computing,SpGEMM,Sparse Matrix Multiplication},
month = {nov},
publisher = {Association for Computing Machinery},
title = {{Optimizing General Sparse Matrix-Matrix Multiplication on the GPU}},
url = {https://doi.org/10.1145/3774654},
year = {2025}
}
@article{Zeng2023,
abstract = {As multicore processors become more common in today's computing systems and parallel programming models are enriched, programmers must consider how to choose the appropriate and parallel programming model when writing parallel code. The purpose of this paper is to compare and analyze the performance gap between different C++ parallel programming models, such as C++ standard library threads, OpenMP and Pthreads, in terms of matrix operations. The experiments use different libraries to implement matrix multiplication separately and then analyze their performance. The experimental data show that the data size has a significant impact on the performance of the different models. For very small matrices of size magnitude less than or close to the number of threads, the performance of parallel implementations is much lower than that of serial implementations. For small matrices with magnitudes larger than the number of threads, the C++ standard library threads outperforms Pthreads and OpenMP due to its lightweight thread performance on relatively small matrices. pthreads shows the best performance on very large matrices due to its fine-grained control over thread management, communication, and synchronization operations. openMP's is not as stable as the other two libraries, especially for smaller matrices. This paper provides a comparative analysis that can help programmers choose the most appropriate library for their specific computational needs.},
author = {Zeng, Guang},
doi = {10.1088/1742-6596/2646/1/012027},
file = {:C\:/Users/ASUS/Downloads/Referensi/Zeng_2023_J._Phys.__Conf._Ser._2646_012027.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng - 2023 - Performance analysis of parallel programming models for C.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
keywords = {Matrix operations,OpenMP,Parallel programming},
number = {1},
title = {{Performance analysis of parallel programming models for C++}},
volume = {2646},
year = {2023}
}
@article{Nigro2022,
abstract = {K-means is a well-known clustering algorithm often used for its simplicity and potential efficiency. Its properties and limitations have been investigated by many works reported in the literature. K-means, though, suffers from computational problems when dealing with large datasets with many dimensions and great number of clusters. Therefore, many authors have proposed and experimented different techniques for the parallel execution of K-means. This paper describes a novel approach to parallel K-means which, today, is based on commodity multicore machines with shared memory. Two reference implementations in Java are developed and their performances are compared. The first one is structured according to a map/reduce schema that leverages the built-in multithreaded concurrency automatically provided by Java to parallel streams. The second one, allocated on the available cores, exploits the parallel programming model of the Theatre actor system, which is control-based, totally lock-free, and purposely relies on threads as coarse-grain “programmingin-the-large” units. The experimental results confirm that some good execution performance can be achieved through the implicit and intuitive use of Java concurrency in parallel streams. However, better execution performance can be guaranteed by the modular Theatre implementation which proves more adequate for an exploitation of the computational resources.},
author = {Nigro, Libero},
doi = {10.3390/a15040117},
file = {:C\:/Users/ASUS/Downloads/Referensi/algorithms-15-00117.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nigro - 2022 - Performance of Parallel K-Means Algorithms in Java.pdf:pdf},
issn = {19994893},
journal = {Algorithms},
keywords = {Java,K-means clustering,actors,functional parallel streams,lightweight parallel programming,message-passing,multi-core machines,parallel algorithms},
number = {4},
title = {{Performance of Parallel K-Means Algorithms in Java}},
volume = {15},
year = {2022}
}
@article{10.1145/3649812,
abstract = {This paper documents a year-long experiment to “profile” the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process. We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust. Over 13 months, 62,526 readers answered questions 1,140,202 times. First, we analyze the trajectories of readers. We find that many readers drop-out of the book early when faced with difficult language concepts like Rust's ownership types. Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions. We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles. Third, we performed 12 interventions into the book to help readers with difficult questions. We find that on average, interventions improved quiz scores on the targeted questions by +20\%. Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N. These results demonstrate that quizzes are a simple and useful technique for understanding language learning at all scales.},
address = {New York, NY, USA},
author = {Crichton, Will and Krishnamurthi, Shriram},
doi = {10.1145/3649812},
file = {:C\:/Users/ASUS/Downloads/Referensi/3649812.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crichton, Krishnamurthi - 2024 - Profiling Programming Language Learning.pdf:pdf},
journal = {Proc. ACM Program. Lang.},
keywords = {digital textbooks,item response theory,rust education},
month = {apr},
number = {OOPSLA1},
publisher = {Association for Computing Machinery},
title = {{Profiling Programming Language Learning}},
url = {https://doi.org/10.1145/3649812},
volume = {8},
year = {2024}
}
@article{10.1145/3434299,
abstract = {Because of its many desirable properties, such as its ability to control effects and thus potentially disastrous race conditions, functional programming offers a viable approach to programming modern multicore computers. Over the past decade several parallel functional languages, typically based on dialects of ML and Haskell, have been developed. These languages, however, have traditionally underperformed procedural languages (such as C and Java). The primary reason for this is their hunger for memory, which only grows with parallelism, causing traditional memory management techniques to buckle under increased demand for memory. Recent work opened a new angle of attack on this problem by identifying a memory property of determinacy-race-free parallel programs, called disentanglement, which limits the knowledge of concurrent computations about each other's memory allocations. The work has showed some promise in delivering good time scalability. In this paper, we present provably space-efficient automatic memory management techniques for determinacy-race-free functional parallel programs, allowing both pure and imperative programs where memory may be destructively updated. We prove that for a program with sequential live memory of R*, any P-processor garbage-collected parallel run requires at most O(R* {\textperiodcentered} P) memory. We also prove a work bound of O(W+R*P) for P-processor executions, accounting also for the cost of garbage collection. To achieve these results, we integrate thread scheduling with memory management. The idea is to coordinate memory allocation and garbage collection with thread scheduling decisions so that each processor can allocate memory without synchronization and independently collect a portion of memory by consulting a collection policy, which we formulate. The collection policy is fully distributed and does not require communicating with other processors. We show that the approach is practical by implementing it as an extension to the MPL compiler for Parallel ML. Our experimental results confirm our theoretical bounds and show that the techniques perform and scale well.},
address = {New York, NY, USA},
author = {Arora, Jatin and Westrick, Sam and Acar, Umut A},
doi = {10.1145/3434299},
file = {:C\:/Users/ASUS/Downloads/Referensi/3434299.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Westrick, Acar - 2021 - Provably space-efficient parallel functional programming.pdf:pdf},
journal = {Proc. ACM Program. Lang.},
keywords = {disentanglement,functional programming,memory management,parallel computing},
month = {jan},
number = {POPL},
publisher = {Association for Computing Machinery},
title = {{Provably space-efficient parallel functional programming}},
url = {https://doi.org/10.1145/3434299},
volume = {5},
year = {2021}
}
@article{JARLOW2025105134,
abstract = {The frequent elements problem, a key component in demanding stream-data analytics, involves selecting elements whose occurrence exceeds a user-specified threshold. Fast, memory-efficient ϵ-approximate synopsis algorithms select all frequent elements but may overestimate them depending on ϵ (user-defined parameter). Evolving applications demand performance only achievable by parallelization. However, algorithmic guarantees concerning concurrent updates and queries have been overlooked. We propose Query and Parallelism Optimized Space-Saving (QPOPSS ), providing concurrency guarantees. A cornerstone of the design is a new approach for the main data structure for the Space-Saving algorithm, enabling support of very fast queries. QPOPSS combines minimal overlap with concurrent updates, distributing work and using fine-grained thread synchronization to achieve high throughput, accuracy, and low memory use. Our analysis shows space and approximation bounds under various concurrency and data distribution conditions. Our empirical evaluation relative to representative state-of-the-art methods reveals that QPOPSS 's multithreaded throughput scales linearly while maintaining the highest accuracy, with orders of magnitude smaller memory footprint.},
author = {Jarlow, Victor and Stylianopoulos, Charalampos and Papatriantafilou, Marina},
doi = {https://doi.org/10.1016/j.jpdc.2025.105134},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731525001017-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarlow, Stylianopoulos, Papatriantafilou - 2025 - QPOPSS Query and Parallelism Optimized Space-Saving for finding frequent stream elemen.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
pages = {105134},
title = {{QPOPSS: Query and Parallelism Optimized Space-Saving for finding frequent stream elements}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525001017},
volume = {204},
year = {2025}
}
@article{Gustafson1988,
author = {Gustafson, John L},
doi = {10.1145/42411.42415},
file = {:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustafson - 1988 - Reevaluating amdahl's law.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustafson - 1988 - Reevaluating amdahl's law(2).pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {5},
pages = {532--533},
title = {{Reevaluating amdahl's law}},
volume = {31},
year = {1988}
}
@article{Jung2021,
author = {Jung, Ralf and Jourdan, Jacques Henri and Krebbers, Robbert and Dreyer, Derek},
doi = {10.1145/3418295},
file = {:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung et al. - 2021 - Safe systems programming in Rust.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung et al. - 2021 - Safe systems programming in Rust(2).pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {4},
pages = {144--152},
title = {{Safe systems programming in Rust}},
volume = {64},
year = {2021}
}
@article{Liu2021,
abstract = { Modern “safe” programming languages follow a design principle that we call safety by default and performance by choice . By default, these languages enforce important programming abstractions, such as memory and type safety, but they also provide mechanisms that allow expert programmers to explicitly trade some safety guarantees for increased performance. However, these same languages have adopted the inverse design principle in their support for multithreading. By default, multithreaded programs violate important abstractions, such as program order and atomic access to individual memory locations to admit compiler and hardware optimizations that would otherwise need to be restricted. Not only does this approach conflict with the design philosophy of safe languages, but very little is known about the practical performance cost of providing a stronger default semantics.  In this article, we propose a safe-by-default and performance-by-choice multithreading semantics for safe languages, which we call  volatile  -by-default . Under this semantics, programs have sequential consistency (SC) by default, which is the natural “interleaving” semantics of threads. However, the volatile -by-default design also includes annotations that allow expert programmers to avoid the associated overheads in performance-critical code. We describe the design, implementation, optimization, and evaluation of the volatile -by-default semantics for two different safe languages: Java and Julia. First, we present V BD-HotSpot and V BDA-HotSpot, modifications of Oracle's HotSpot JVM that enforce the volatile -by-default semantics on Intel x86-64 hardware and ARM-v8 hardware. Second, we present S C-Julia, a modification to the just-in-time compiler within the standard Julia implementation that provides best-effort enforcement of the volatile -by-default semantics on x86-64 hardware for the purpose of performance evaluation. We also detail two different implementation techniques: a baseline approach that simply reuses existing mechanisms in the compilers for handling atomic accesses, and a speculative approach that avoids the overhead of enforcing the volatile -by-default semantics until there is the possibility of an SC violation. Our results show that the cost of enforcing SC is significant but arguably still acceptable for some use cases today. Further, we demonstrate that compiler optimizations as well as programmer annotations can reduce the overhead considerably. },
author = {Liu, Lun and Millstein, Todd and Musuvathi, Madanlal},
doi = {10.1145/3462206},
file = {:C\:/Users/ASUS/Downloads/Referensi/3462206.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Millstein, Musuvathi - 2021 - Safe-by-default Concurrency for Modern Programming Languages.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Millstein, Musuvathi - 2021 - Safe-by-default Concurrency for Modern Programming Languages(2).pdf:pdf},
issn = {0164-0925},
journal = {ACM Transactions on Programming Languages and Systems},
number = {3},
pages = {1--50},
title = {{Safe-by-default Concurrency for Modern Programming Languages}},
volume = {43},
year = {2021}
}
@article{SCHRYEN2024104835,
abstract = {In high performance computing environments, we observe an ongoing increase in the available number of cores. For example, the current TOP500 list reveals that nine clusters have more than 1 million cores. This development calls for re-emphasizing performance (scalability) analysis and speedup laws as suggested in the literature (e.g., Amdahl's law and Gustafson's law), with a focus on asymptotic performance. Understanding speedup and efficiency issues of algorithmic parallelism is useful for several purposes, including the optimization of system operations, temporal predictions on the execution of a program, the analysis of asymptotic properties, and the determination of speedup bounds. However, the literature is fragmented and shows a large diversity and heterogeneity of speedup models and laws. These phenomena make it challenging to obtain an overview of the models and their relationships, to identify the determinants of performance in a given algorithmic and computational context, and, finally, to determine the applicability of performance models and laws to a particular parallel computing setting. In this work, I provide a generic speedup (and thus also efficiency) model for homogeneous computing environments. My approach generalizes many prominent models suggested in the literature and allows showing that they can be considered special cases of a unifying approach. The genericity of the unifying speedup model is achieved through parameterization. Considering combinations of parameter ranges, I identify six different asymptotic speedup cases and eight different asymptotic efficiency cases. Jointly applying these speedup and efficiency cases, I derive eleven scalability cases, from which I build a scalability typology. Researchers can draw upon my suggested typology to classify their speedup model and to determine the asymptotic behavior when the number of parallel processing units increases. Also, the description of two computational experiments demonstrates the practical application of the model and the typology. In addition, my results may be used and extended in future research to address various extensions of my setting.},
author = {Schryen, Guido},
doi = {https://doi.org/10.1016/j.jpdc.2023.104835},
file = {:C\:/Users/ASUS/Downloads/Referensi/1-s2.0-S0743731523002058-main.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schryen - 2024 - Speedup and efficiency of computational parallelization A unifying approach and asymptotic analysis.pdf:pdf},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Asymptotic analysis,Efficiency,Performance analysis,Scalability,Speedup},
pages = {104835},
title = {{Speedup and efficiency of computational parallelization: A unifying approach and asymptotic analysis}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731523002058},
volume = {187},
year = {2024}
}
@book{Klabnik2018,
author = {Klabnik, Steve and Nichols, Carol},
edition = {2},
file = {:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klabnik, Nichols - 2018 - The Rust Programming Language.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klabnik, Nichols - 2023 - The Rust Programming Language.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klabnik, Nichols - 2023 - The Rust Programming Language(2).pdf:pdf},
isbn = {1593278519},
publisher = {No Starch Press},
title = {{The Rust Programming Language}},
year = {2023}
}
@article{Cheeseman2023,
abstract = {Expressing parallelism and coordination is central for modern concurrent programming. Many mechanisms exist for expressing both parallelism and coordination. However, the design decisions for these two mechanisms are tightly intertwined. We believe that the interdependence of these two mechanisms should be recognised and achieved through a single, powerful primitive. We are not the first to realise this: the prime example is actor model programming, where parallelism arises through fine-grained decomposition of a program's state into actors that are able to execute independently in parallel. However, actor model programming has a serious pain point: updating multiple actors as a single atomic operation is a challenging task. We address this pain point by introducing a new concurrency paradigm: Behaviour-Oriented Concurrency (BoC). In BoC, we are revisiting the fundamental concept of a behaviour to provide a more transactional concurrency model. BoC enables asynchronously creating atomic and ordered units of work with exclusive access to a collection of independent resources. In this paper, we describe BoC informally in terms of examples, which demonstrate the advantages of exclusive access to several independent resources, as well as the need for ordering. We define it through a formal model. We demonstrate its practicality by implementing a C++ runtime. We argue its applicability through the Savina benchmark suite: benchmarks in this suite can be more compactly represented using BoC in place of Actors, and we observe comparable, if not better, performance.},
author = {Cheeseman, Luke and Parkinson, Matthew J. and Clebsch, Sylvan and Kogias, Marios and Drossopoulou, Sophia and Chisnall, David and Wrigstad, Tobias and Li{\'{e}}tar, Paul},
doi = {10.1145/3622852},
file = {:C\:/Users/ASUS/Downloads/Referensi/3622852.pdf:pdf;:C\:/Users/ASUS/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheeseman et al. - 2023 - When Concurrency Matters Behaviour-Oriented Concurrency.pdf:pdf},
issn = {24751421},
journal = {Proceedings of the ACM on Programming Languages},
keywords = {actors},
number = {OOPSLA2},
title = {{When Concurrency Matters: Behaviour-Oriented Concurrency}},
volume = {7},
year = {2023}
}
